+++
title = "What Fast-Food Order Screens Can Teach Us About Broken Observability"
date = "2025-07-12"
description = "What fast-food order screens reveal about broken observability in engineering, and how to build dashboards that actually help."
tags = ["observability", "devops", "engineering-culture"]
+++

{{< foldergallery src="observability-fastfood-img">}}

Ever stood at the most popular fast-food chain in Singapore (where, in my opinion, this issue feels especially pronounced), staring at the giant digital order screen that supposedly tells you when your food is ready? It looks slickâ€”numbers updating in real time, animations flyingâ€”but youâ€™re still standing there five minutes after your number disappeared. Or your food is ready before it even shows up. The systemâ€™s workingâ€”but no one believes it anymore.

Thatâ€™s exactly the kind of trap we fall into with observability in engineering teams.

## When Observability Exists But Doesnâ€™t Help

We all know when monitoring is missingâ€”thatâ€™s obvious. But there's a sneakier problem: dashboards, alerts, and metrics that exist but donâ€™t actually help anyone.

Hereâ€™s the usual setup:

1. **Telemetry Collection & Storage**  
   Youâ€™ve got your Prometheus, OTEL, or some vendor agent scraping metrics from apps, infra, and services. Cool. But collecting data doesnâ€™t mean people know what to do with it.

2. **Dashboards & Visualization**  
   This is where a lot of us screw up. We throw every chart we can think of into Grafanaâ€”CPU, memory, HTTP codes, request rates. Technically correct, but not what folks actually need when things are on fire. What we really care about is stuff like â€œare users able to log in?â€ or â€œare purchases going through?â€

3. **Alerts & Incident Response**  
   Without context and clear ownership, alerts turn into background noise. Nobody wants to be paged for a spike that recovers in 30 seconds. If the alert doesnâ€™t help fix or prevent something, itâ€™s just noise.

## The Fast-Food Display Syndrome

Back to the food court analogy: the screen is up and running, but it doesnâ€™t match reality. The staff are doing their thing, orders are flying, but whatâ€™s shown on screen has little to do with whatâ€™s actually happening. People start ignoring it.

Same thing happens to your dashboards if theyâ€™re disconnected from what your system or users are really doing. Trust dies. And without trust, the whole observability setup is just noise and sunk cost.

- ğŸ› ï¸ **Slow Incident Response**: People rely on MS Teams chats and tribal knowledge instead of the data.
- ğŸ“¦ **Guesswork on Scaling**: Infra gets over- or under-provisioned because weâ€™re not using real signals.
- ğŸ§ª **Performance Slips Under the Radar**: Gradual regressions sneak past everyone.
- ğŸ“‰ **Nobody Maintains Dashboards**: They rot because nobody's using them.

## How to Build Observability Engineers Actually Use

Letâ€™s get back to basics:

### 1. Start With What Actually Matters
Think like a user. Whatâ€™s the failure that makes people angry? Start from there, then trace it to signals your system produces. Track those.

- Instead of counting 500s, measure failed checkouts or dropped login attempts.
- Donâ€™t just chart latencyâ€”track whether youâ€™re meeting your SLA per endpoint or function.

### 2. Cut the Noise
Prometheus can scrape everything, but that doesnâ€™t mean it should. Keep dashboards tight and focused. Each one should have a reason to exist.

- One â€œmainâ€ dashboard per service.
- Keep it clean. Only show what SREs or devs actually look at during incidents.

### 3. Make Alerts Useful Again
Good alerts are rare. They tell the right team whatâ€™s wrong, why it matters, and what to do next. If an alert doesnâ€™t meet that bar, itâ€™s a log line pretending to be useful.

### 4. Tune Constantly
Check whoâ€™s acknowledging alerts. When was the last time a dashboard was opened? If nobodyâ€™s using a panel, archive it or fix it.

### 5. Make Observability Part of the Routine
Donâ€™t wait for a P1 to look at dashboards. Bring them into standups, retro, postmortems. Ask â€œwhat wouldâ€™ve made this easier to catch?â€

## Final Thought: Are You Building Control Towers or Eye Candy?

Observability should help you *run* your systemsâ€”not just decorate your wiki or status board. If your team doesnâ€™t trust your metrics or ignores your dashboards, thatâ€™s not just an ops issueâ€”itâ€™s a broken feedback loop.

Before you add another panel or scrape job, ask:  
> â€œIs anyone using the stuff we already have?â€

If not, youâ€™re just building another fast-food displayâ€”looks cool, tells you nothing.